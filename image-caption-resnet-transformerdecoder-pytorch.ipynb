{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter \nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport math\nimport torch.nn.functional as F\nimport pickle\nimport gc\nimport random\npd.set_option('display.max_colwidth', None)\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read Data.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/image-caption/train_set_1 (1).csv\")\nprint(len(df))\ndisplay(df.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.read_csv(\"../input/image-caption/train_set_2 (1).csv\")\nprint(len(df2))\ndisplay(df2.head(3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing -> Remove Single Character and non alpha Words. Add <Start>, <end> and <pad> tokens. <pad> token is appended such that length in max_seq_len (maximum length across all captions which is 33 in our case)  ","metadata":{}},{"cell_type":"code","source":"def remove_single_char_word(word_list):\n    lst = []\n    for word in word_list:\n        if len(word)>1:\n            lst.append(word)\n\n    return lst","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['cleaned_caption'] = df['Caption'].apply(lambda Caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in Caption.split(\" \")] + ['<end>'])\ndf['cleaned_caption']  = df['cleaned_caption'].apply(lambda x : remove_single_char_word(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['cleaned_caption'] = df2['Caption'].apply(lambda Caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in Caption.split(\" \")] + ['<end>'])\ndf2['cleaned_caption']  = df2['cleaned_caption'].apply(lambda x : remove_single_char_word(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['seq_len'] = df['cleaned_caption'].apply(lambda x : len(x))\nmax_seq_len = df['seq_len'].max()\nprint(max_seq_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['seq_len'] = df2['cleaned_caption'].apply(lambda x : len(x))\nmax_seq_len = df2['seq_len'].max()\nprint(max_seq_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['seq_len'], axis = 1, inplace = True)\ndf['cleaned_caption'] = df['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.drop(['seq_len'], axis = 1, inplace = True)\ndf2['cleaned_caption'] = df2['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df.head(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df2.head(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Vocab and mapping of token to ID","metadata":{}},{"cell_type":"code","source":"word_list = df['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\nword_dict = Counter(word_list)\nword_dict =  sorted(word_dict, key=word_dict.get, reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_list2 = df2['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\nword_dict2 = Counter(word_list2)\nword_dict2 =  sorted(word_dict2, key=word_dict2.get, reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(word_dict))\nprint(word_dict[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(word_dict2))\nprint(word_dict2[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vocab size is 8360","metadata":{}},{"cell_type":"code","source":"vocab_size = len(word_dict)\nprint(vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size2 = len(word_dict2)\nprint(vocab_size2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_to_word = {index: word for index, word in enumerate(word_dict)}\nword_to_index = {word: index for index, word in enumerate(word_dict)}\nprint(len(index_to_word), len(word_to_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_to_word2 = {index: word for index, word in enumerate(word_dict2)}\nword_to_index2 = {word: index for index, word in enumerate(word_dict2)}\nprint(len(index_to_word2), len(word_to_index2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Covert sequence of tokens to IDs","metadata":{}},{"cell_type":"code","source":"df['text_seq']  = df['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['text_seq']  = df2['cleaned_caption'].apply(lambda caption : [word_to_index2[word] for word in caption] )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df.head(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df2.head(2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split In Train and validation data. Same Image should not be present in both training and validation data ","metadata":{}},{"cell_type":"code","source":"df = df.sort_values(by = 'image')\ntrain = df.iloc[:int(0.9*len(df))]\nvalid = df.iloc[int(0.9*len(df)):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df2.sort_values(by = 'image')\ntrain2 = df2.iloc[:int(0.9*len(df2))]\nvalid2 = df2.iloc[int(0.9*len(df2)):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"print(len(train), train['id'].nunique())\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T08:16:14.856324Z","iopub.execute_input":"2022-02-20T08:16:14.856625Z","iopub.status.idle":"2022-02-20T08:16:14.869038Z","shell.execute_reply.started":"2022-02-20T08:16:14.856595Z","shell.execute_reply":"2022-02-20T08:16:14.868151Z"}}},{"cell_type":"code","source":"print(len(train), train['image'].nunique())\nprint(len(valid), valid['image'].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train2), train2['image'].nunique())\nprint(len(valid2), valid2['image'].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract features from Images Using Resnet","metadata":{}},{"cell_type":"code","source":"train_samples = len(train)\nprint(train_samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_samples2 = len(train2)\nprint(train_samples2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unq_train_imgs = train[['image']].drop_duplicates()\nunq_valid_imgs = valid[['image']].drop_duplicates()\nprint(len(unq_train_imgs), len(unq_valid_imgs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unq_train_imgs2 = train2[['image']].drop_duplicates()\nunq_valid_imgs2 = valid2[['image']].drop_duplicates()\nprint(len(unq_train_imgs2), len(unq_valid_imgs2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class extractImageFeatureResNetDataSet():\n    def __init__(self, data):\n        self.data = data \n        self.scaler = transforms.Resize([224, 224])\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n    def __len__(self):  \n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        image_name = self.data.iloc[idx]['image']\n        img_loc = '../input/image-caption/Train 1-20220219T110320Z-001/Train 1/'+str((image_name))\n\n        img = Image.open(img_loc).convert('RGB')\n        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n\n        return image_name, t_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class extractImageFeatureResNetDataSet2():\n    def __init__(self, data):\n        self.data = data \n        self.scaler = transforms.Resize([224, 224])\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n    def __len__(self):  \n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        image_name = self.data.iloc[idx]['image']\n        img_loc = '../input/image-caption/Train 2-20220219T110322Z-001/Train 2/'+str((image_name))\n        img = Image.open(img_loc).convert('RGB')\n        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n\n        return image_name, t_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\ntrain_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ImageDataset_ResNet2 = extractImageFeatureResNetDataSet2(unq_train_imgs2)\ntrain_ImageDataloader_ResNet2 = DataLoader(train_ImageDataset_ResNet2, batch_size = 1, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\nvalid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ImageDataset_ResNet2 = extractImageFeatureResNetDataSet2(unq_valid_imgs2)\nvalid_ImageDataloader_ResNet2 = DataLoader(valid_ImageDataset_ResNet2, batch_size = 1, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet18 = torchvision.models.resnet18(pretrained=True).to(device)\nresnet18.eval()\nlist(resnet18._modules)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resNet18Layer4 = resnet18._modules.get('layer4').to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_vector(t_img):\n    \n    t_img = Variable(t_img)\n    my_embedding = torch.zeros(1, 512, 7, 7)\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n    \n    h = resNet18Layer4.register_forward_hook(copy_data)\n    resnet18(t_img)\n    \n    h.remove()\n    return my_embedding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_train = {}\nfor image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n    \n    extract_imgFtr_ResNet_train[image_name[0]] = embdg\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_valid = {}\nfor image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n \n    extract_imgFtr_ResNet_valid[image_name[0]] = embdg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_file = open(\"./EncodedImageValidResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_valid, a_file)\na_file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_file = open(\"./EncodedImageTrainResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_train, a_file)\na_file.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_train2 = {}\nfor image_name, t_img in tqdm(train_ImageDataloader_ResNet2):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n    \n    extract_imgFtr_ResNet_train2[image_name[0]] = embdg\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_file2 = open(\"./EncodedImageTrainResNet2.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_train2, a_file2)\na_file2.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_valid2 = {}\nfor image_name, t_img in tqdm(valid_ImageDataloader_ResNet2):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n \n    extract_imgFtr_ResNet_valid2[image_name[0]] = embdg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_file2 = open(\"./EncodedImageValidResNet2.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_valid2, a_file2)\na_file2.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create DataLoader which will be used to load data into Transformer Model.\n## FlickerDataSetResnet will return caption sequence, 1 timestep left shifted caption sequence which model will predict and Stored Image features from ResNet.","metadata":{}},{"cell_type":"code","source":"class FlickerDataSetResnet():\n    def __init__(self, data, pkl_file):\n        self.data = data\n        self.encodedImgs = pd.read_pickle(pkl_file)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n    \n        caption_seq = self.data.iloc[idx]['text_seq']\n        target_seq = caption_seq[1:]+[0]\n\n        image_name   = self.data.iloc[idx]['image']\n        image_tensor = self.encodedImgs[image_name]\n        image_tensor = image_tensor.permute(0,2,3,1)\n        image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n\n        return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\ntrain_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size = 32, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\nvalid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size = 32, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_resnet2 = FlickerDataSetResnet(train2, 'EncodedImageTrainResNet2.pkl')\ntrain_dataloader_resnet2 = DataLoader(train_dataset_resnet2, batch_size = 32, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_dataset_resnet2 = FlickerDataSetResnet(valid2, 'EncodedImageValidResNet2.pkl')\nvalid_dataloader_resnet2 = DataLoader(valid_dataset_resnet2, batch_size = 32, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Transformer Decoder Model. This Model will take caption sequence and the extracted resnet image features as input and ouput 1 timestep shifted (left) caption sequence. \n## In the Transformer decoder, lookAhead and padding mask has also been applied","metadata":{}},{"cell_type":"markdown","source":"### Position Embedding","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=max_seq_len):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n\n    def forward(self, x):\n        if self.pe.size(0) < x.size(0):\n            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n        self.pe = self.pe[:x.size(0), : , : ]\n        \n        x = x + self.pe\n        return self.dropout(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Decoder","metadata":{}},{"cell_type":"code","source":"class ImageCaptionModel(nn.Module):\n    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n        super(ImageCaptionModel, self).__init__()\n        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n        self.embedding_size = embedding_size\n        self.embedding = nn.Embedding(vocab_size , embedding_size)\n        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.last_linear_layer.bias.data.zero_()\n        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)\n\n    def generate_Mask(self, size, decoder_inp):\n        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n\n        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n        decoder_input_pad_mask_bool = decoder_inp == 0\n\n        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n\n    def forward(self, encoded_image, decoder_inp):\n        encoded_image = encoded_image.permute(1,0,2)\n        \n\n        decoder_inp_embed = self.embedding(decoder_inp)* math.sqrt(self.embedding_size)\n        \n        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n        \n\n        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(decoder_inp.size(1), decoder_inp)\n        decoder_input_mask = decoder_input_mask.to(device)\n        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n        \n\n        decoder_output = self.TransformerDecoder(tgt = decoder_inp_embed, memory = encoded_image, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n        \n        final_output = self.last_linear_layer(decoder_output)\n\n        return final_output,  decoder_input_pad_mask\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Train the Model","metadata":{}},{"cell_type":"markdown","source":"### The cross entropy loss has been masked at time steps where input token is <'pad'>.","metadata":{}},{"cell_type":"code","source":"EPOCH = 180","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ictModel = ImageCaptionModel(16, 4, vocab_size, 512).to(device)\noptimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.00001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\ncriterion = torch.nn.CrossEntropyLoss(reduction='none')\nmin_val_loss = np.float('Inf')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(EPOCH)):\n    total_epoch_train_loss = 0\n    total_epoch_valid_loss = 0\n    total_train_words = 0\n    total_valid_words = 0\n    ictModel.train()\n\n    ### Train Loop\n    for caption_seq, target_seq, image_embed in train_dataloader_resnet:\n\n        optimizer.zero_grad()\n\n        image_embed = image_embed.squeeze(1).to(device)\n        caption_seq = caption_seq.to(device)\n        target_seq = target_seq.to(device)\n\n        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n        output = output.permute(1, 2, 0)\n\n        loss = criterion(output,target_seq)\n\n        loss_masked = torch.mul(loss, padding_mask)\n\n        final_batch_loss = torch.sum(loss_masked)/torch.sum(padding_mask)\n\n        final_batch_loss.backward()\n        optimizer.step()\n        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n        total_train_words += torch.sum(padding_mask)\n\n \n    total_epoch_train_loss = total_epoch_train_loss/total_train_words\n  \n\n    ### Eval Loop\n    model = (torch.load(\"../input/imahecap/BestModel2\"))\n    model.eval()\n    ictModel.eval()\n    with torch.no_grad():\n        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n\n            image_embed = image_embed.squeeze(1).to(device)\n            caption_seq = caption_seq.to(device)\n            target_seq = target_seq.to(device)\n\n            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n            output = output.permute(1, 2, 0)\n\n            loss = criterion(output,target_seq)\n\n            loss_masked = torch.mul(loss, padding_mask)\n\n            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n            total_valid_words += torch.sum(padding_mask)\n\n    total_epoch_valid_loss = total_epoch_valid_loss/total_valid_words\n  \n    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n  \n    if min_val_loss > total_epoch_valid_loss:\n        print(\"Writing Model at epoch \", epoch)\n        torch.save(ictModel, './BestModel')\n        min_val_loss = total_epoch_valid_loss\n  \n\n    scheduler.step(total_epoch_valid_loss.item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = (torch.load(\"../input/imahecap/BestModel2\"))\nmodel.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(EPOCH)):\n    total_epoch_train_loss = 0\n    total_epoch_valid_loss = 0\n    total_train_words = 0\n    total_valid_words = 0\n    ictModel.train()\n\n    ### Train Loop\n    for caption_seq, target_seq, image_embed in train_dataloader_resnet2:\n\n        optimizer.zero_grad()\n\n        image_embed = image_embed.squeeze(1).to(device)\n        caption_seq = caption_seq.to(device)\n        target_seq = target_seq.to(device)\n\n        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n        output = output.permute(1, 2, 0)\n\n        loss = criterion(output,target_seq)\n\n        loss_masked = torch.mul(loss, padding_mask)\n\n        final_batch_loss = torch.sum(loss_masked)/torch.sum(padding_mask)\n\n        final_batch_loss.backward()\n        optimizer.step()\n        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n        total_train_words += torch.sum(padding_mask)\n\n \n    total_epoch_train_loss = total_epoch_train_loss/total_train_words\n  \n\n    ### Eval Loop\n    ictModel.eval()\n    with torch.no_grad():\n        for caption_seq, target_seq, image_embed in valid_dataloader_resnet2:\n\n            image_embed = image_embed.squeeze(1).to(device)\n            caption_seq = caption_seq.to(device)\n            target_seq = target_seq.to(device)\n\n            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n            output = output.permute(1, 2, 0)\n\n            loss = criterion(output,target_seq)\n\n            loss_masked = torch.mul(loss, padding_mask)\n\n            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n            total_valid_words += torch.sum(padding_mask)\n\n    total_epoch_valid_loss = total_epoch_valid_loss/total_valid_words\n  \n    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n  \n    if min_val_loss > total_epoch_valid_loss:\n        print(\"Writing Model at epoch \", epoch)\n        torch.save(ictModel, './BestModel')\n        min_val_loss = total_epoch_valid_loss\n  \n\n    scheduler.step(total_epoch_valid_loss.item())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.load('./BestModel')\nstart_token = word_to_index['<start>']\nend_token = word_to_index['<end>']\npad_token = word_to_index['<pad>']\nmax_seq_len = 40\nprint(start_token, end_token, pad_token)\npridiction = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dft = pd.read_csv(\"../input/image-caption/sample2.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class extractImageFeatureResNetDataSett():\n    def __init__(self, data):\n        self.data = data \n        self.scaler = transforms.Resize([224, 224])\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n    def __len__(self):  \n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        image_name = self.data.iloc[idx]['id']\n        img_loc = '../input/image-caption/Test-20220219T111319Z-001/Test/'+str((image_name))\n\n        img = Image.open(img_loc).convert(\"RGB\")\n        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n\n        return image_name, t_img","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ImageDataset_ResNet = extractImageFeatureResNetDataSett(dft)\ntest_ImageDataloader_ResNet = DataLoader(test_ImageDataset_ResNet, batch_size = 1, shuffle=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_test = {}\nfor image_name, t_img in tqdm(test_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n    \n    extract_imgFtr_ResNet_test[image_name[0]] = embdg","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a_file = open(\"./testImageValidResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_test, a_file)\na_file.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_embed = pd.read_pickle('testImageValidResNet.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_caption(k, img_nm): \n\n    model.eval() \n    img_embed = test_img_embed[img_nm].to(device)\n\n\n    img_embed = img_embed.permute(0,2,3,1)\n    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n\n    \n    input_seq = [pad_token]*max_seq_len\n    input_seq[0] = start_token\n\n    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n    predicted_sentence = []\n    with torch.no_grad():\n        for eval_iter in range(0, max_seq_len):\n\n            output, padding_mask = model.forward(img_embed, input_seq)\n\n            output = output[eval_iter, 0, :]\n\n            values = torch.topk(output, k).values.tolist()\n            indices = torch.topk(output, k).indices.tolist()\n\n            next_word_index = random.choices(indices, values, k = 1)[0]\n\n            next_word = index_to_word[next_word_index]\n\n            input_seq[:, :eval_iter+1] = next_word_index\n\n\n            if next_word == '<end>' :\n                break\n\n            predicted_sentence.append(next_word)\n    listToStr = ' '.join(map(str,predicted_sentence))\n    print(\"\\n\")\n    print(listToStr)\n    pridiction.append(listToStr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"let = len(dft)\nprint(let)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.read_csv(\"../input/image-caption/sample.csv\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(let):\n    generate_caption(1, dft.iloc[i]['id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pridiction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2[\"Caption\"] = pridiction\ndf2 = df2[[\"id\", \"Caption\"]]\ndf2.to_csv(\"submission.csv\", index=False)\ndf2.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}